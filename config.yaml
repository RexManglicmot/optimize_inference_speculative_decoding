# config.yaml

project:
  name: "specdec_inference_opt"
  seed: 42
  input_dir: "./data/processed"             # where PubMedQA subset or README lives
  output_dir: "./outputs"         # all results/plots written here
  save_raw_outputs: false       # set true to save raw_generations.jsonl (slower)
  save_metrics: true
  save_plots: true

dataset:
  source: "csv"
  path: "./data/processed/my_data.csv"
  text_fields:
    id: "id"
    question: "question"
    abstract: "contexts"         # alias PubMedQA "context" â†’ abstract
  prompt_template: |
    Q: {question}
    Contexts: {abstract}
    A:
  sample_size: 200
  max_input_tokens: 512
  chunking:
    enabled: false
    window_tokens: 896
    overlap_tokens: 128

models:
  drafts:
    - id: "distilbert/distilgpt2" # distilgpt2.....hmmmmmm
    - id: "openai-community/gpt2"
    - id: "EleutherAI/gpt-neo-125M"
    - id: "EleutherAI/gpt-neo-1.3B"
    - id: "openai-community/gpt2-medium"
  verifier:
    id: "EleutherAI/gpt-j-6B"
    precision: "fp16"
  device: "cuda"                # Vast.ai GPU (CUDA, fp16)

decoding:
  speculative_enabled: true
  draft_k: 6                    # increased to keep GPU busy
  max_new_tokens: 256           # longer generations sustain GPU kernels
  temperature: 0.7
  top_p: 0.9
  stop_on_newline: false

batching:
  enabled: true
  batch_size: 8                 # bump to 16 if VRAM allows

evaluation:
  latency:
    enabled: true
    percentiles: [50, 95]
  throughput:
    enabled: true
  acceptance_rate:
    enabled: true
  baseline:
    run_verifier_only: true     # needed for speedup/disagreement metrics
  quality:
    perplexity: false
    bleu: false
    semantic_similarity: false
  cost:
    enabled: true
    price_per_1k_tokens_usd: 0.002  # example; adjust if you want to model $ cost

plots:
  latency_bar: true
  throughput_bar: true
  acceptance_bar: true
  disagreement_bar: true
  speedup_bar: true
  latency_vs_accept_scatter: false
  cost_bar: false

logging:
  level: "INFO"
  progress_every: 10
