# config.yaml

project:
  name: "specdec_inference_opt"
  seed: 42
  input_dir: "./data/processed"             # where PubMedQA subset or README lives
  output_dir: "./outputs"         # all results/plots written here
  save_raw_outputs: false       # set true to save raw_generations.jsonl (slower)
  save_metrics: true
  save_plots: true

dataset:
  source: "csv"
  path: "./data/processed/my_data.csv"
  text_fields:
    id: "id"
    question: "question"
    abstract: "contexts"         # alias PubMedQA "context" â†’ abstract
  prompt_template: |
    Q: {question}
    Contexts: {abstract}
    A:
  sample_size: 0 #want all so use 0 # it was 200
  max_input_tokens: 512
  chunking:
    enabled: false
    window_tokens: 896
    overlap_tokens: 128

models:
  drafts:
    - id: "distilgpt2" # distilgpt2.....hmmmmmm
    - id: "openai-community/gpt2"
    # - id: "EleutherAI/gpt-neo-125M"
    - id: "openai-community/gpt2-large"
    - id: "openai-community/gpt2-medium"
  verifier:
    id: "openai-community/gpt2-xl" # was EleutherAI/gpt-j-6B
    precision: "bf16" # was "fp16"
  device: "cuda"                # Vast.ai GPU (CUDA, fp16)

decoding:
  speculative_enabled: true
  draft_k: 64 #32 #16 # was 6 before                    # increased to keep GPU busy
  max_new_tokens: 256 # longer generations sustain GPU kernels
  temperature: 0.7
  context_max_tokens: 512 # new
  batch_size: 16   # new
  top_p: 0.9
  stop_on_newline: false

batching:
  enabled: true
  batch_size: 8                 # bump to 16 if VRAM allows

evaluation:
  latency:
    enabled: true
    percentiles: [50, 95]
  throughput:
    enabled: true
  acceptance_rate:
    enabled: true
  baseline:
    run_verifier_only: true     # needed for speedup/disagreement metrics
  
plots:
  latency_bar: true
  throughput_bar: true
  acceptance_bar: true
  speedup_bar: true

logging:
  level: "INFO"
  progress_every: 10


